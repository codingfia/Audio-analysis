# Audio-analysis

This a project focused on Speech Emotion Recognition using LSTM and CNN. We aim to recognize human emotion and affective states from speech. 

Datasets used in this projectï¼š

Crowd-sourced Emotional Mutimodal Actors Dataset (Crema-D)<br>
Ryerson Audio-Visual Database of Emotional Speech and Song (Ravdess)<br>
Surrey Audio-Visual Expressed Emotion (Savee)<br>
Toronto emotional speech set (Tess)<br>

We recognize seven emotions (angry, fear, disgust, sad, happy, neutral, surprise) from 4 speech datasets, composed of 10239 speech samples, with 97% accuracy of the CNN model.
